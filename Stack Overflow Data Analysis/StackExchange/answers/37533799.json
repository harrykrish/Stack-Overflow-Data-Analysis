{"title": "Why is statistics.mean() so slow?", "tags": ["python", "performance", "mean"], "creation_date": 1464643350, "score": 65, "body": "<p>Python's <code>statistics</code> module is not built for speed, but for precision</p>\n\n<p>In <a href=\"https://www.python.org/dev/peps/pep-0450/\" rel=\"nofollow\">the specs for this module</a>, it appears that</p>\n\n<blockquote>\n  <p>The built-in sum can lose accuracy when dealing with floats of wildly\n  differing magnitude.  Consequently, the above naive mean fails this\n  \"torture test\"</p>\n  \n  <p><code>assert mean([1e30, 1, 3, -1e30]) == 1</code></p>\n  \n  <p>returning 0 instead of 1, a purely computational error of 100%.</p>\n  \n  <p>Using math.fsum inside mean will make it more accurate with float\n  data, but it also has the side-effect of converting any arguments to\n  float even when unnecessary.  E.g. we should expect the mean of a list\n  of Fractions to be a Fraction, not a float.</p>\n</blockquote>\n\n<p>Conversely, if we take a look at the implementation of <code>_sum()</code> in this module, the first lines of the method's docstring <a href=\"https://hg.python.org/cpython/file/3.5/Lib/statistics.py#l119\" rel=\"nofollow\">seem to confirm that</a>:</p>\n\n<pre><code>def _sum(data, start=0):\n    \"\"\"_sum(data [, start]) -&gt; (type, sum, count)\n\n    Return a high-precision sum of the given numeric data as a fraction,\n    together with the type to be converted to and the count of items.\n\n    [...] \"\"\"\n</code></pre>\n\n<p>So yeah, <code>statistics</code> implementation of <code>sum</code>, instead of being a simple one-liner call to Python's built-in <code>sum()</code> function, takes about 20 lines by itself with a nested <code>for</code> loop in its body.</p>\n\n<p>This happens because <code>statistics._sum</code> chooses to guarantee the maximum precision for all types of number it could encounter (even if they widely differ from one another), instead of simply emphasizing speed.</p>\n\n<p>Hence, it appears normal that the built-in <code>sum</code> proves a hundred times faster. The cost of it being a much lower precision in you happen to call it with exotic numbers.</p>\n\n<p><strong>Other options</strong></p>\n\n<p>If you need to prioritize speed in your algorithms, you should have a look at <a href=\"http://www.numpy.org/\" rel=\"nofollow\">Numpy</a> instead, the algorithms of which being implemented in C.</p>\n\n<p>NumPy mean is not as precise as <code>statistics</code> by a long shot but it implements (since 2013) a <a href=\"https://github.com/numpy/numpy/pull/3685\" rel=\"nofollow\">routine based on pairwise summation</a> which is better than a naive <code>sum/len</code> (more info in the link).</p>\n\n<p>However...</p>\n\n<pre><code>import numpy as np\nimport statistics\n\nnp_mean = np.mean([1e30, 1, 3, -1e30])\nstatistics_mean = statistics.mean([1e30, 1, 3, -1e30])\n\nprint('NumPy mean: {}'.format(np_mean))\nprint('Statistics mean: {}'.format(statistics_mean))\n\n&gt; NumPy mean: 0.0\n&gt; Statistics mean: 1.0\n</code></pre>\n", "last_activity_date": 1477691060, "answer_id": 37533799, "is_accepted": true, "owner": {"user_id": 2091169, "reputation": 5037, "user_type": "registered", "accept_rate": 79, "display_name": "Jivan", "link": "http://stackoverflow.com/users/2091169/jivan", "profile_image": "https://i.stack.imgur.com/MT8tZ.jpg?s=128&g=1"}, "last_edit_date": 1477691060, "question_id": 37533666}