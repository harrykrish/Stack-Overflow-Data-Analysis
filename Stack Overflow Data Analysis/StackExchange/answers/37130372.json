{"title": "ClassNotFoundException spark-submit scala", "tags": ["scala", "apache-spark"], "creation_date": 1462860102, "score": 2, "body": "<p>For a Spark job to run, it need to self-replicate it's code over the different nodes that make part of your spark cluster. It does that by literally copying over the jar file to the other nodes.</p>\n\n<p>That means that you need to make sure that your class files are packaged in a .jar file. In my typical solutions, I would build an Uber jar that packages the class files, and the dependent jar files together in a single .jar file. For that I use the <a href=\"https://maven.apache.org/plugins/maven-shade-plugin/\" rel=\"nofollow\">Maven Shade plugin</a>. That doesn't have to be your solution, but at least you should build a .jar file out of your generated classes.</p>\n\n<p>To provide manually additional jar files - you will need to add them using the <code>--jars</code> option which will expect a comma delimited list.</p>\n\n<h2>Update 1</h2>\n\n<p>Actually, even for me there is a lot of confusion about all the available options, specifically to the jar files and how they are distributed, or modify the classpath in spark. <a href=\"http://stackoverflow.com/questions/37132559/add-jars-to-a-spark-job-spark-submit\">See another topic I just posted</a>.</p>\n\n<h2>Update 2</h2>\n\n<p>For the second part of your question that is already answered on <a href=\"http://stackoverflow.com/questions/27925375/nosuchmethoderror-when-declaring-a-variable\">another thread</a>.</p>\n", "last_activity_date": 1462867934, "answer_id": 37130372, "is_accepted": true, "owner": {"user_id": 744133, "reputation": 2394, "user_type": "registered", "accept_rate": 60, "display_name": "YoYo", "link": "http://stackoverflow.com/users/744133/yoyo", "profile_image": "https://i.stack.imgur.com/z21Fe.jpg?s=128&g=1"}, "last_edit_date": 1462867934, "question_id": 37130178}