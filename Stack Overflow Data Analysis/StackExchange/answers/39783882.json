{"title": "Does Adding A Value Differ In Computational Requirement From Subtracting A Value?", "tags": ["optimization"], "creation_date": 1475210965, "score": 0, "body": "<p>You'd need to specify a particular environment, down to the hardware to get a precise answer, but we can certainly still make some general observations.</p>\n\n<p>On most modern hardware, addition and subtraction take the same amount of time and use essentially the same shared hardware to implement both operations. Practically all common hardware today uses <a href=\"https://en.wikipedia.org/wiki/Two%27s_complement\" rel=\"nofollow\">twos-complement arithmetic</a>, and any unit capable of <em>addition</em> in twos complement arithmetic can perform subtraction, since either input can be negative. In particular, <code>A - B</code> is equivalent to <code>A + (-B)</code>, so you can perform subtraction using an addition unit by negating the second argument. In practice, this is as simple as inverting (logical <code>NOT</code>) the argument, and setting the carry in for the lowest order bit.</p>\n\n<p>The upshot is that modern laptop, desktop, phone, server, etc, hardware generally performs both addition and subtraction in 1 cycle, and even older or more obscure chips that take multiple cycles usually took the same time for both.</p>\n\n<p>That's the hardware side. It's possible though that <em>software</em> could mess things up, e.g., by making subtraction slower than addition - but that's unlikely.</p>\n\n<p>Lower level languages like C, C++, Java and so on generally map their fixed-size integral types in a straightforward way to the facilities offered by hardware and so addition and subtraction end up taking the same time. Even when these languages offer integral types that the hardware can't support directly (e.g., 64-bit <code>long</code> on a 32-bit platform), the small software methods or inlined code is generally identical for the addition and subtraction paths.</p>\n\n<p>Higher level languages often implement something like a <a href=\"https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic\" rel=\"nofollow\"><em>bignum</em></a> for numeric types - a type whose range has no particular limit and expands in size as needed. Here it is <em>conceivable</em> that subtraction is slower than addition. E.g., there may be a highly optimized bignum addition routine, and then subtraction may be implemented in terms of that addition routine by flipping the sign on the second argument, making subtraction slightly slower.</p>\n\n<p>Looking at one example of a <em>bignum</em> however, we see that the implementations are essentially symmetric: the Java 8 <code>BigInteger.subtract()</code> <a href=\"http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/tip/src/share/classes/java/math/BigInteger.java#l1409\" rel=\"nofollow\">implementation</a> is doing <a href=\"http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/tip/src/share/classes/java/math/BigInteger.java#l1207\" rel=\"nofollow\">the same thing</a> as <code>add()</code>.</p>\n\n<p>Overall, the ubiquity of two's complement arithmetic in hardware, combined with reasonable language implementations means that both addition and subtractions are likely to have identical performance characteristics. </p>\n\n<p>Most of the above applies to floating point addition as well: on modern machines, these take the same time.</p>\n\n<p>There are, however, many differences between <em>signed</em> and <em>unsigned</em> arithmetic - but that's a whole other topic. </p>\n", "last_activity_date": 1475211433, "answer_id": 39783882, "is_accepted": true, "owner": {"user_id": 149138, "reputation": 6036, "user_type": "registered", "accept_rate": 67, "display_name": "BeeOnRope", "link": "http://stackoverflow.com/users/149138/beeonrope", "profile_image": "https://www.gravatar.com/avatar/cfd457233c8ebbab383475fc097442d9?s=128&d=identicon&r=PG"}, "last_edit_date": 1475211433, "question_id": 39783659}