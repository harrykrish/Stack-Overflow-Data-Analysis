{"title": "Count number of NULL values in a row of Dataframe table in Apache Spark using Scala", "tags": ["scala", "apache-spark", "apache-spark-sql", "spark-dataframe"], "creation_date": 1458226487, "score": 1, "body": "<p>Test date:</p>\n\n<pre><code>case class Document( a: String, b: String, c: String)\nval df = sc.parallelize(Seq(new Document(null, null, null), new Document(\"a\", null, null), new Document(\"a\", \"b\", null), new Document(\"a\", \"b\", \"c\"), new Document(null, null, \"c\"))).df\n</code></pre>\n\n<p><strong>With UDF</strong></p>\n\n<p>Remixing the answer by <a href=\"http://stackoverflow.com/users/4856939/david-griffin\">David</a> and my RDD version below, you can do it using a UDF that takes a row:</p>\n\n<pre><code>def nullFilter = udf((x:Row) =&gt; {Range(0, x.length).count(x.isNullAt(_)) &lt; 2})\ndf.filter(nullFilter(struct(df.columns.map(df(_)) : _*))).show\n</code></pre>\n\n<p><strong>With RDD</strong></p>\n\n<p>You could turn it into a rdd, loop of the columns in the Row and count how many are null.</p>\n\n<pre><code>sqlContext.createDataFrame(df.rdd.filter( x=&gt; Range(0, x.length).count(x.isNullAt(_)) &lt; 2 ), df.schema).show\n</code></pre>\n", "last_activity_date": 1458229405, "answer_id": 36064152, "is_accepted": true, "owner": {"user_id": 157672, "reputation": 11314, "user_type": "registered", "accept_rate": 91, "display_name": "mlk", "link": "http://stackoverflow.com/users/157672/mlk", "profile_image": "https://www.gravatar.com/avatar/3f78727411a1318799ba42cdcb0360f0?s=128&d=identicon&r=PG"}, "last_edit_date": 1458229405, "question_id": 36062908}