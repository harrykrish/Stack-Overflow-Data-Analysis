{"body": "<p>Although a year later, but might help others..</p>\n\n<p>As you said, a full cartesian product is insane in your case. Your matching records will be close in time (5 minutes) so you can take advantage of that and save a lot of time if you first group together records to buckets based on their timestamp, then join the two dataframes on that bucket and only then apply the filter. Using that method causes Spark to use a SortMergeJoin and not a CartesianProduct and greatly boosts performance.</p>\n\n<p>There is a small caveat here - you must match to both the bucket and the next one.</p>\n\n<p>It's better explain in my blog, with working code examples (Scala + Spark 2.0 but you can implement the same in python too...)</p>\n\n<p><a href=\"http://zachmoshe.com/2016/09/26/efficient-range-joins-with-spark.html\" rel=\"nofollow\">http://zachmoshe.com/2016/09/26/efficient-range-joins-with-spark.html</a></p>\n", "tags": ["join", "apache-spark", "apache-spark-sql", "pyspark"], "creation_date": 1475188806, "score": 1, "last_activity_date": 1475188806, "answer_id": 39781092, "is_accepted": false, "owner": {"user_id": 1643257, "reputation": 758, "user_type": "registered", "accept_rate": 67, "display_name": "Zach Moshe", "link": "http://stackoverflow.com/users/1643257/zach-moshe", "profile_image": "https://i.stack.imgur.com/s5qJB.jpg?s=128&g=1"}, "title": "Joining two spark dataframes on time (TimestampType) in python", "question_id": 30630296}